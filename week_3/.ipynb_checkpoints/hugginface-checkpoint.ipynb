{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a4e0ff-4870-47b8-96ba-3114b4fb995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "     ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/126.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/126.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/126.8 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/126.8 kB 145.2 kB/s eta 0:00:01\n",
      "     ----------- ------------------------- 41.0/126.8 kB 140.3 kB/s eta 0:00:01\n",
      "     ----------------- ------------------- 61.4/126.8 kB 192.5 kB/s eta 0:00:01\n",
      "     ----------------------- ------------- 81.9/126.8 kB 241.3 kB/s eta 0:00:01\n",
      "     ------------------------------- ---- 112.6/126.8 kB 297.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 126.8/126.8 kB 287.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp311-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "   ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/8.2 MB 2.3 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.1/8.2 MB 2.2 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.1/8.2 MB 2.2 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.2/8.2 MB 893.0 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.2/8.2 MB 1.1 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.2/8.2 MB 1.1 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.2/8.2 MB 1.1 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/8.2 MB 873.8 kB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.5/8.2 MB 1.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.5/8.2 MB 1.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/8.2 MB 1.1 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.7/8.2 MB 1.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.8/8.2 MB 1.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.2 MB 1.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.2 MB 1.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.2 MB 1.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.2 MB 1.3 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.0/8.2 MB 1.1 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.4/8.2 MB 1.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.4/8.2 MB 1.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.5/8.2 MB 1.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.7/8.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.3/8.2 MB 2.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.4/8.2 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/8.2 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/8.2 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/8.2 MB 2.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/8.2 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.2/8.2 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/8.2 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.5/8.2 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.7/8.2 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.8/8.2 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.0/8.2 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.2 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/8.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.6/8.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.8/8.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/8.2 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.1/8.2 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.3/8.2 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.5/8.2 MB 2.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.6/8.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.8/8.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.9/8.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.1/8.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.2/8.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.4/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.5/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.8/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.9/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.1/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.3/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.4/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.8/8.2 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.0/8.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.2/8.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.2/8.2 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.3 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 204.8/330.3 kB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  327.7/330.3 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 330.3/330.3 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp311-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.5 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 174.1/277.5 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  276.5/277.5 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.5/277.5 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.2/2.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 4.8 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.8/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "   ---------------------------------------- 0.0/169.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 169.0/169.0 kB 5.1 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2023.12.2 huggingface-hub-0.20.2 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36bd8047-f2a6-4bfd-b04b-3df34a617cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (4.66.1)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/977.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/977.5 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/977.5 kB 187.9 kB/s eta 0:00:06\n",
      "     - ----------------------------------- 30.7/977.5 kB 187.9 kB/s eta 0:00:06\n",
      "     - ----------------------------------- 30.7/977.5 kB 187.9 kB/s eta 0:00:06\n",
      "     -- ---------------------------------- 61.4/977.5 kB 204.8 kB/s eta 0:00:05\n",
      "     -- ---------------------------------- 71.7/977.5 kB 218.6 kB/s eta 0:00:05\n",
      "     ---- ------------------------------- 112.6/977.5 kB 312.2 kB/s eta 0:00:03\n",
      "     ---- ------------------------------- 122.9/977.5 kB 313.8 kB/s eta 0:00:03\n",
      "     ----- ------------------------------ 153.6/977.5 kB 353.1 kB/s eta 0:00:03\n",
      "     ------- ---------------------------- 204.8/977.5 kB 414.8 kB/s eta 0:00:02\n",
      "     -------- --------------------------- 235.5/977.5 kB 450.6 kB/s eta 0:00:02\n",
      "     --------- -------------------------- 256.0/977.5 kB 462.8 kB/s eta 0:00:02\n",
      "     ---------- ------------------------- 286.7/977.5 kB 478.3 kB/s eta 0:00:02\n",
      "     ------------ ----------------------- 337.9/977.5 kB 524.3 kB/s eta 0:00:02\n",
      "     ------------- ---------------------- 368.6/977.5 kB 533.3 kB/s eta 0:00:02\n",
      "     -------------- --------------------- 399.4/977.5 kB 541.2 kB/s eta 0:00:02\n",
      "     ---------------- ------------------- 450.6/977.5 kB 587.1 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 573.4/977.5 kB 706.6 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 706.6/977.5 kB 825.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 737.3/977.5 kB 830.9 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 788.5/977.5 kB 844.1 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 901.1/977.5 kB 919.4 kB/s eta 0:00:01\n",
      "     -----------------------------------  972.8/977.5 kB 962.6 kB/s eta 0:00:01\n",
      "     ------------------------------------ 977.5/977.5 kB 953.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from tqdm>=4.27->transformers[sentencepiece]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers[sentencepiece]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers[sentencepiece]) (2023.11.17)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf477e8-6d02-4c15-b328-6c57025702b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af4da5-aaac-4446-ad54-a42cd39dcb42",
   "metadata": {},
   "source": [
    "# **Transformers**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c4cd8-41f7-4409-94c7-669f02e84bbb",
   "metadata": {},
   "source": [
    "### Working with pipelines\r",
    "Pipeline function returns an end-to-end object that performs an NLP task on one or several texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13bdbd83-6a4d-4f99-b92a-c136d5ce1de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 629/629 [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 268M/268M [00:59<00:00, 4.49MB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 48.0/48.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|█████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 553kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9972789883613586},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "    \"I don't like apples!\",\n",
    "    \"I hate this so much!\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f73d8-3c65-4cd0-a9f0-4a3c23b25012",
   "metadata": {},
   "source": [
    "- This pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English.\r\n",
    "- The model is downloaded and cached when you create the classifier object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63aee8-5f4f-49d0-8f0a-35e9547e5350",
   "metadata": {},
   "source": [
    "Some of the currently available pipelines are:\r\n",
    "\r\n",
    "- feature-extraction (get the vector representation of a text)\r\n",
    "- fill-mask\r\n",
    "- ner (named entity recognition)\r\n",
    "- question-answering\r\n",
    "- sentiment-analysis\r\n",
    "- summarization\r\n",
    "- text-generation\r\n",
    "- translation\r\n",
    "- zero-shot-classification\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666981e-7e8c-4260-aa23-78c1a96f6422",
   "metadata": {},
   "source": [
    "# **Zero-shot Classification**\n",
    "- Classify texts that haven’t been labelled.\r\n",
    "- For this use case, the zero-shot-classification pipeline is very powerful.\r\n",
    "  - It allows you to specify which labels to use for the classification, so you don’t have to rely on the labels of the pretrained model.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be588e8e-42c6-4bfd-afbe-a6123714d43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445989489555359, 0.11197426170110703, 0.04342678561806679]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5028ecd3-f81d-4cdb-bb01-a45298690b9a",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "- The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text.\r\n",
    "- The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f21fc3-04e4-4141-bbb0-df4f634eba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'WE will learn about Nepal\\'s current and future economic situation and be able to bring us together to help build a sustainable future for Nepal, our neighbors and those we serve on the continent,\" he said.\\n\\n\"We will work to implement a comprehensive'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"WE will learn about Nepal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac387e-87b8-4b52-a8b5-b2f65840f4f3",
   "metadata": {},
   "source": [
    "### Using any model from the Hub in a pipeline\r",
    "Let’s try the distilgpt2 model! Here’s how to load it in the same pipeline as before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ee3169c-86d0-4d2d-b05f-27085a1d388f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 762/762 [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 353M/353M [01:24<00:00, 4.18MB/s]\n",
      "generation_config.json: 100%|█████████████████████████████████████████████████████████████████| 124/124 [00:00<?, ?B/s]\n",
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 2.30MB/s]\n",
      "merges.txt: 100%|████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 642kB/s]\n",
      "tokenizer.json: 100%|██████████████████████████████████████████████████████████████| 1.36M/1.36M [00:04<00:00, 318kB/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use the tools, and how they could allow you to do both. We have only published a few'},\n",
       " {'generated_text': 'In this course, we will teach you how to set up a self-help website and start implementing the best practices. The goal, however, is'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e5611-3109-4f7e-9ba6-40aa1d97d446",
   "metadata": {},
   "source": [
    "#### The Interference API\n",
    "All the models can be tested directly through our browser using the Inference API, which is available on the Hugging Face website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe032d4-bdc7-4463-bbab-4664343ba4fb",
   "metadata": {},
   "source": [
    "### Mask filling\n",
    "\n",
    "The idea of this task is to fill in the blanks in a given text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d1f65ef-2976-4910-8de4-61cf1bc1aa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 480/480 [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 331M/331M [01:17<00:00, 4.29MB/s]\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "vocab.json: 100%|███████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 1.04MB/s]\n",
      "merges.txt: 100%|███████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 1.19MB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 1.45MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1961977779865265,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.04052717983722687,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c59a6a-6581-44cc-947a-a2ebf5e6dbd7",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36fa7c69-1515-4940-834a-5a09f8c01c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 998/998 [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████████████████████████████████████████████████████| 1.33G/1.33G [05:20<00:00, 4.16MB/s]\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 60.0/60.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|█████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 531kB/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Aavash and I work at Hugging Face in Nepal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0a1b6-3475-43f9-9044-901c5df07652",
   "metadata": {},
   "source": [
    "### Question answering\n",
    "The question-answering pipeline answers questions using information from a given context\n",
    "\n",
    "Note that this pipeline works by extracting information from the provided context; it does not generate the answer.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b47ee554-5586-4d3b-8bd9-e24249bc5b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 473/473 [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 261M/261M [01:05<00:00, 3.98MB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 29.0/29.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 3.71MB/s]\n",
      "tokenizer.json: 100%|████████████████████████████████████████████████████████████████| 436k/436k [00:00<00:00, 738kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9582499861717224, 'start': 32, 'end': 44, 'answer': 'Fusemachines'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Aavash and I work in Fusemachines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820fa6d-806c-4106-801c-fcfc542e0211",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a93f8eb9-5b3b-4e82-a959-50062ae12c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████| 1.80k/1.80k [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "pytorch_model.bin: 100%|██████████████████████████████████████████████████████████| 1.22G/1.22G [04:57<00:00, 4.11MB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████| 26.0/26.0 [00:00<00:00, 13.0kB/s]\n",
      "vocab.json: 100%|███████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 1.07MB/s]\n",
      "merges.txt: 100%|████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 800kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The number of engineering graduates in the United States has declined in recent years . China and India graduate six and eight times as many traditional engineers as the U.S. does . Rapidly developing economies such as China continue to encourage and advance the teaching of engineering . There are declining offerings in engineering subjects dealing with infrastructure, infrastructure, the environment, and related issues .'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of\n",
    "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "    the premier American universities engineering curricula now concentrate on\n",
    "    and encourage largely the study of engineering science. As a result, there\n",
    "    are declining offerings in engineering subjects dealing with infrastructure,\n",
    "    the environment, and related issues, and greater concentration on high\n",
    "    technology subjects, largely supporting increasingly complex scientific\n",
    "    developments. While the latter is important, it should not be at the expense\n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other\n",
    "    industrial countries in Europe and Asia, continue to encourage and advance\n",
    "    the teaching of engineering. Both China and India, respectively, graduate\n",
    "    six and eight times as many traditional engineers as does the United States.\n",
    "    Other industrial countries at minimum maintain their output, while America\n",
    "    suffers an increasingly serious decline in the number of engineering graduates\n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd23f6-c560-448f-8f39-050c44eedaa8",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fdb4e78-3c28-4faa-92c9-479668460625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|████████████████████████████████████████████████████████████████████████| 1.42k/1.42k [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "pytorch_model.bin: 100%|████████████████████████████████████████████████████████████| 301M/301M [01:13<00:00, 4.07MB/s]\n",
      "generation_config.json: 100%|██████████████████████████████████████████████████████████| 293/293 [00:00<00:00, 294kB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 42.0/42.0 [00:00<?, ?B/s]\n",
      "source.spm: 100%|███████████████████████████████████████████████████████████████████| 802k/802k [00:00<00:00, 4.31MB/s]\n",
      "target.spm: 100%|███████████████████████████████████████████████████████████████████| 778k/778k [00:00<00:00, 3.09MB/s]\n",
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████| 1.34M/1.34M [00:00<00:00, 4.06MB/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f244c-1971-4b42-ba81-8ecd2bc8bdab",
   "metadata": {},
   "source": [
    "### History\n",
    "![History of transformers](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a563f91-d7a2-432e-a15b-ed11ea1fc858",
   "metadata": {},
   "source": [
    "The Transformer architecture was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models.\r\n",
    "\r\n",
    "Broadly, they can be grouped into three categories:\r\n",
    "\r\n",
    "- GPT-like (also called auto-regressive Transformer models)\r\n",
    "- BERT-like (also called auto-encoding Transformer models)\r\n",
    "- BART/T5-like (also called sequence-to-sequence Transformer models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7caf58f-a0e8-4375-8173-09a712c834cb",
   "metadata": {},
   "source": [
    "### Transformers are language models\n",
    "\n",
    "All the Transformer models have been trained on large amounts of raw text in a self-supervised fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model.\n",
    "\n",
    "This type of model develops a statistical understanding of the language it has been trained on, but it’s not very useful for specific practical tasks.\n",
    "\n",
    "Because of this, the general pretrained model then goes through a process called transfer learning. During this process, the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task.\n",
    "\n",
    "**predicting the next word**\n",
    "\n",
    "This is called causal language modeling.\n",
    "\n",
    "![Causal language modeling](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg\n",
    "                           Another example is masked language modeling, in which the model predicts a masked word in the sentence.\n",
    "\n",
    "![Masked Language Modeling](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdefac-33e6-4983-bfc7-3164b75863d2",
   "metadata": {},
   "source": [
    "Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the models’ sizes as well as the amount of data they are pretrained on.\n",
    "\n",
    "![Transformers are big](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png)\n",
    "\n",
    "g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db07dbd2-1a5c-4c7b-a4ae-b8a3846b614a",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "***Pretraining*** is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.\n",
    "\n",
    "***Fine-tuning***, on the other hand, is the training done after a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task.\n",
    "\n",
    "### General Architecture\n",
    "\n",
    "    The model is primarily composed of two blocks:\n",
    "\n",
    "- Encoder (left): The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.\n",
    "- Decoder (right): The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.\n",
    "\n",
    "![Architecture](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e959ad-eac7-4d32-bfae-1e6004a7b5a6",
   "metadata": {},
   "source": [
    "Each of these parts can be used independently, depending on the task:\n",
    "\n",
    "- **Encoder-only models**: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
    "- **Decoder-only models**: Good for generative tasks such as text generation.\n",
    "- Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7219d-4013-4b7b-a46b-56b4402170a0",
   "metadata": {},
   "source": [
    "### Attention layers\n",
    "This layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.\n",
    "\n",
    "### The original Architecture\n",
    "\n",
    "- The Transformer architecture was originally designed for translation.\n",
    "- During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language.\n",
    "- In the encoder, the attention layers can use all the words in a sentence\n",
    "- The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a5853-95d1-41a4-bda9-513192bf7173",
   "metadata": {},
   "source": [
    "- When the model has access to target sentences, the decoder is fed the whole target, but it is not allowed to use future words\n",
    "\n",
    "![Transformer Architecture](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg)\n",
    "\n",
    "The *attention mask* can also be used in the encoder/decoder to prevent the model from paying attention to some special words — for instance, the special padding word used to make all the inputs the same length when batching together sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c40e9-2e05-47ee-8d0d-b6db707de6b4",
   "metadata": {},
   "source": [
    "### Architectures vs. checkpoints\n",
    "\n",
    "- **Architecture**: This is the skeleton of the model — the definition of each layer and each operation that happens within the model.\n",
    "- **Checkpoints**: These are the weights that will be loaded in a given architecture.\n",
    "- **Model**: This is an umbrella term that isn’t as precise as “architecture” or “checkpoint”: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity.\n",
    "\n",
    "For example, BERT is an architecture while bert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say “the BERT model” and “the bert-base-cased model.”\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ccd464-49da-4442-a68e-902f5496d9fc",
   "metadata": {},
   "source": [
    "While transformer models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "133cd84a-afe1-4d12-8005-ea3b870f2266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f96206-d673-4854-92dc-e1a79124c5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
