{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a4e0ff-4870-47b8-96ba-3114b4fb995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "     ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/126.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/126.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/126.8 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/126.8 kB 145.2 kB/s eta 0:00:01\n",
      "     ----------- ------------------------- 41.0/126.8 kB 140.3 kB/s eta 0:00:01\n",
      "     ----------------- ------------------- 61.4/126.8 kB 192.5 kB/s eta 0:00:01\n",
      "     ----------------------- ------------- 81.9/126.8 kB 241.3 kB/s eta 0:00:01\n",
      "     ------------------------------- ---- 112.6/126.8 kB 297.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 126.8/126.8 kB 287.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp311-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "   ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/8.2 MB 2.3 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.1/8.2 MB 2.2 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.1/8.2 MB 2.2 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.2/8.2 MB 893.0 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.2/8.2 MB 1.1 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.2/8.2 MB 1.1 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.2/8.2 MB 1.1 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/8.2 MB 873.8 kB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.5/8.2 MB 1.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.5/8.2 MB 1.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/8.2 MB 1.1 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.7/8.2 MB 1.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.8/8.2 MB 1.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.2 MB 1.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.2 MB 1.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.2 MB 1.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.2 MB 1.3 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.0/8.2 MB 1.1 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.4/8.2 MB 1.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.4/8.2 MB 1.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.5/8.2 MB 1.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.7/8.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.3/8.2 MB 2.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.4/8.2 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/8.2 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/8.2 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/8.2 MB 2.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/8.2 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.2/8.2 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/8.2 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.5/8.2 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.7/8.2 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.8/8.2 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.0/8.2 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.2 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/8.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.6/8.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.8/8.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/8.2 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.1/8.2 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.3/8.2 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.5/8.2 MB 2.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.6/8.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.8/8.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.9/8.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.1/8.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.2/8.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.4/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.5/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.8/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.9/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.1/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.3/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.4/8.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.8/8.2 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.0/8.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.2/8.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.2/8.2 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.3 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 204.8/330.3 kB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  327.7/330.3 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 330.3/330.3 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp311-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.5 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 174.1/277.5 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  276.5/277.5 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.5/277.5 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.2/2.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 4.8 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.8/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "   ---------------------------------------- 0.0/169.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 169.0/169.0 kB 5.1 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2023.12.2 huggingface-hub-0.20.2 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36bd8047-f2a6-4bfd-b04b-3df34a617cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (4.66.1)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/977.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/977.5 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/977.5 kB 187.9 kB/s eta 0:00:06\n",
      "     - ----------------------------------- 30.7/977.5 kB 187.9 kB/s eta 0:00:06\n",
      "     - ----------------------------------- 30.7/977.5 kB 187.9 kB/s eta 0:00:06\n",
      "     -- ---------------------------------- 61.4/977.5 kB 204.8 kB/s eta 0:00:05\n",
      "     -- ---------------------------------- 71.7/977.5 kB 218.6 kB/s eta 0:00:05\n",
      "     ---- ------------------------------- 112.6/977.5 kB 312.2 kB/s eta 0:00:03\n",
      "     ---- ------------------------------- 122.9/977.5 kB 313.8 kB/s eta 0:00:03\n",
      "     ----- ------------------------------ 153.6/977.5 kB 353.1 kB/s eta 0:00:03\n",
      "     ------- ---------------------------- 204.8/977.5 kB 414.8 kB/s eta 0:00:02\n",
      "     -------- --------------------------- 235.5/977.5 kB 450.6 kB/s eta 0:00:02\n",
      "     --------- -------------------------- 256.0/977.5 kB 462.8 kB/s eta 0:00:02\n",
      "     ---------- ------------------------- 286.7/977.5 kB 478.3 kB/s eta 0:00:02\n",
      "     ------------ ----------------------- 337.9/977.5 kB 524.3 kB/s eta 0:00:02\n",
      "     ------------- ---------------------- 368.6/977.5 kB 533.3 kB/s eta 0:00:02\n",
      "     -------------- --------------------- 399.4/977.5 kB 541.2 kB/s eta 0:00:02\n",
      "     ---------------- ------------------- 450.6/977.5 kB 587.1 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 573.4/977.5 kB 706.6 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 706.6/977.5 kB 825.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 737.3/977.5 kB 830.9 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 788.5/977.5 kB 844.1 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 901.1/977.5 kB 919.4 kB/s eta 0:00:01\n",
      "     -----------------------------------  972.8/977.5 kB 962.6 kB/s eta 0:00:01\n",
      "     ------------------------------------ 977.5/977.5 kB 953.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from transformers[sentencepiece]) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from tqdm>=4.27->transformers[sentencepiece]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers[sentencepiece]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aavash\\anaconda3\\envs\\research\\lib\\site-packages (from requests->transformers[sentencepiece]) (2023.11.17)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf477e8-6d02-4c15-b328-6c57025702b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af4da5-aaac-4446-ad54-a42cd39dcb42",
   "metadata": {},
   "source": [
    "# **Transformers**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c4cd8-41f7-4409-94c7-669f02e84bbb",
   "metadata": {},
   "source": [
    "### Working with pipelines\r",
    "Pipeline function returns an end-to-end object that performs an NLP task on one or several texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13bdbd83-6a4d-4f99-b92a-c136d5ce1de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 629/629 [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 268M/268M [00:59<00:00, 4.49MB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 48.0/48.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|█████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 553kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9972789883613586},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "    \"I don't like apples!\",\n",
    "    \"I hate this so much!\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f73d8-3c65-4cd0-a9f0-4a3c23b25012",
   "metadata": {},
   "source": [
    "- This pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English.\r\n",
    "- The model is downloaded and cached when you create the classifier object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63aee8-5f4f-49d0-8f0a-35e9547e5350",
   "metadata": {},
   "source": [
    "Some of the currently available pipelines are:\r\n",
    "\r\n",
    "- feature-extraction (get the vector representation of a text)\r\n",
    "- fill-mask\r\n",
    "- ner (named entity recognition)\r\n",
    "- question-answering\r\n",
    "- sentiment-analysis\r\n",
    "- summarization\r\n",
    "- text-generation\r\n",
    "- translation\r\n",
    "- zero-shot-classification\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666981e-7e8c-4260-aa23-78c1a96f6422",
   "metadata": {},
   "source": [
    "# **Zero-shot Classification**\n",
    "- Classify texts that haven’t been labelled.\r\n",
    "- For this use case, the zero-shot-classification pipeline is very powerful.\r\n",
    "  - It allows you to specify which labels to use for the classification, so you don’t have to rely on the labels of the pretrained model.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be588e8e-42c6-4bfd-afbe-a6123714d43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445989489555359, 0.11197426170110703, 0.04342678561806679]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5028ecd3-f81d-4cdb-bb01-a45298690b9a",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "- The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text.\r\n",
    "- The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f21fc3-04e4-4141-bbb0-df4f634eba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'WE will learn about Nepal\\'s current and future economic situation and be able to bring us together to help build a sustainable future for Nepal, our neighbors and those we serve on the continent,\" he said.\\n\\n\"We will work to implement a comprehensive'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"WE will learn about Nepal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac387e-87b8-4b52-a8b5-b2f65840f4f3",
   "metadata": {},
   "source": [
    "### Using any model from the Hub in a pipeline\r",
    "Let’s try the distilgpt2 model! Here’s how to load it in the same pipeline as before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ee3169c-86d0-4d2d-b05f-27085a1d388f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 762/762 [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 353M/353M [01:24<00:00, 4.18MB/s]\n",
      "generation_config.json: 100%|█████████████████████████████████████████████████████████████████| 124/124 [00:00<?, ?B/s]\n",
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 2.30MB/s]\n",
      "merges.txt: 100%|████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 642kB/s]\n",
      "tokenizer.json: 100%|██████████████████████████████████████████████████████████████| 1.36M/1.36M [00:04<00:00, 318kB/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use the tools, and how they could allow you to do both. We have only published a few'},\n",
       " {'generated_text': 'In this course, we will teach you how to set up a self-help website and start implementing the best practices. The goal, however, is'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e5611-3109-4f7e-9ba6-40aa1d97d446",
   "metadata": {},
   "source": [
    "#### The Interference API\n",
    "All the models can be tested directly through our browser using the Inference API, which is available on the Hugging Face website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe032d4-bdc7-4463-bbab-4664343ba4fb",
   "metadata": {},
   "source": [
    "### Mask filling\n",
    "\n",
    "The idea of this task is to fill in the blanks in a given text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d1f65ef-2976-4910-8de4-61cf1bc1aa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 480/480 [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 331M/331M [01:17<00:00, 4.29MB/s]\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "vocab.json: 100%|███████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 1.04MB/s]\n",
      "merges.txt: 100%|███████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 1.19MB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 1.45MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1961977779865265,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.04052717983722687,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c59a6a-6581-44cc-947a-a2ebf5e6dbd7",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36fa7c69-1515-4940-834a-5a09f8c01c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 998/998 [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████████████████████████████████████████████████████| 1.33G/1.33G [05:20<00:00, 4.16MB/s]\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 60.0/60.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|█████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 531kB/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Aavash and I work at Hugging Face in Nepal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0a1b6-3475-43f9-9044-901c5df07652",
   "metadata": {},
   "source": [
    "### Question answering\n",
    "The question-answering pipeline answers questions using information from a given context\n",
    "\n",
    "Note that this pipeline works by extracting information from the provided context; it does not generate the answer.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b47ee554-5586-4d3b-8bd9-e24249bc5b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 473/473 [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 261M/261M [01:05<00:00, 3.98MB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 29.0/29.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 3.71MB/s]\n",
      "tokenizer.json: 100%|████████████████████████████████████████████████████████████████| 436k/436k [00:00<00:00, 738kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9582499861717224, 'start': 32, 'end': 44, 'answer': 'Fusemachines'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Aavash and I work in Fusemachines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820fa6d-806c-4106-801c-fcfc542e0211",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a93f8eb9-5b3b-4e82-a959-50062ae12c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████| 1.80k/1.80k [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "pytorch_model.bin: 100%|██████████████████████████████████████████████████████████| 1.22G/1.22G [04:57<00:00, 4.11MB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████| 26.0/26.0 [00:00<00:00, 13.0kB/s]\n",
      "vocab.json: 100%|███████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 1.07MB/s]\n",
      "merges.txt: 100%|████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 800kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The number of engineering graduates in the United States has declined in recent years . China and India graduate six and eight times as many traditional engineers as the U.S. does . Rapidly developing economies such as China continue to encourage and advance the teaching of engineering . There are declining offerings in engineering subjects dealing with infrastructure, infrastructure, the environment, and related issues .'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of\n",
    "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "    the premier American universities engineering curricula now concentrate on\n",
    "    and encourage largely the study of engineering science. As a result, there\n",
    "    are declining offerings in engineering subjects dealing with infrastructure,\n",
    "    the environment, and related issues, and greater concentration on high\n",
    "    technology subjects, largely supporting increasingly complex scientific\n",
    "    developments. While the latter is important, it should not be at the expense\n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other\n",
    "    industrial countries in Europe and Asia, continue to encourage and advance\n",
    "    the teaching of engineering. Both China and India, respectively, graduate\n",
    "    six and eight times as many traditional engineers as does the United States.\n",
    "    Other industrial countries at minimum maintain their output, while America\n",
    "    suffers an increasingly serious decline in the number of engineering graduates\n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd23f6-c560-448f-8f39-050c44eedaa8",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fdb4e78-3c28-4faa-92c9-479668460625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|████████████████████████████████████████████████████████████████████████| 1.42k/1.42k [00:00<?, ?B/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "pytorch_model.bin: 100%|████████████████████████████████████████████████████████████| 301M/301M [01:13<00:00, 4.07MB/s]\n",
      "generation_config.json: 100%|██████████████████████████████████████████████████████████| 293/293 [00:00<00:00, 294kB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 42.0/42.0 [00:00<?, ?B/s]\n",
      "source.spm: 100%|███████████████████████████████████████████████████████████████████| 802k/802k [00:00<00:00, 4.31MB/s]\n",
      "target.spm: 100%|███████████████████████████████████████████████████████████████████| 778k/778k [00:00<00:00, 3.09MB/s]\n",
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████| 1.34M/1.34M [00:00<00:00, 4.06MB/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f244c-1971-4b42-ba81-8ecd2bc8bdab",
   "metadata": {},
   "source": [
    "### History\n",
    "![History of transformers](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a563f91-d7a2-432e-a15b-ed11ea1fc858",
   "metadata": {},
   "source": [
    "The Transformer architecture was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models.\r\n",
    "\r\n",
    "Broadly, they can be grouped into three categories:\r\n",
    "\r\n",
    "- GPT-like (also called auto-regressive Transformer models)\r\n",
    "- BERT-like (also called auto-encoding Transformer models)\r\n",
    "- BART/T5-like (also called sequence-to-sequence Transformer models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7caf58f-a0e8-4375-8173-09a712c834cb",
   "metadata": {},
   "source": [
    "### Transformers are language models\n",
    "\n",
    "All the Transformer models have been trained on large amounts of raw text in a self-supervised fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model.\n",
    "\n",
    "This type of model develops a statistical understanding of the language it has been trained on, but it’s not very useful for specific practical tasks.\n",
    "\n",
    "Because of this, the general pretrained model then goes through a process called transfer learning. During this process, the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task.\n",
    "\n",
    "**predicting the next word**\n",
    "\n",
    "This is called causal language modeling.\n",
    "\n",
    "![Causal language modeling](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg\n",
    "                           Another example is masked language modeling, in which the model predicts a masked word in the sentence.\n",
    "\n",
    "![Masked Language Modeling](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdefac-33e6-4983-bfc7-3164b75863d2",
   "metadata": {},
   "source": [
    "Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the models’ sizes as well as the amount of data they are pretrained on.\n",
    "\n",
    "![Transformers are big](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png)\n",
    "\n",
    "g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db07dbd2-1a5c-4c7b-a4ae-b8a3846b614a",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "***Pretraining*** is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.\n",
    "\n",
    "***Fine-tuning***, on the other hand, is the training done after a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task.\n",
    "\n",
    "### General Architecture\n",
    "\n",
    "    The model is primarily composed of two blocks:\n",
    "\n",
    "- Encoder (left): The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.\n",
    "- Decoder (right): The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.\n",
    "\n",
    "![Architecture](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e959ad-eac7-4d32-bfae-1e6004a7b5a6",
   "metadata": {},
   "source": [
    "Each of these parts can be used independently, depending on the task:\n",
    "\n",
    "- **Encoder-only models**: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
    "- **Decoder-only models**: Good for generative tasks such as text generation.\n",
    "- Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7219d-4013-4b7b-a46b-56b4402170a0",
   "metadata": {},
   "source": [
    "### Attention layers\n",
    "This layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.\n",
    "\n",
    "### The original Architecture\n",
    "\n",
    "- The Transformer architecture was originally designed for translation.\n",
    "- During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language.\n",
    "- In the encoder, the attention layers can use all the words in a sentence\n",
    "- The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a5853-95d1-41a4-bda9-513192bf7173",
   "metadata": {},
   "source": [
    "- When the model has access to target sentences, the decoder is fed the whole target, but it is not allowed to use future words\n",
    "\n",
    "![Transformer Architecture](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg)\n",
    "\n",
    "The *attention mask* can also be used in the encoder/decoder to prevent the model from paying attention to some special words — for instance, the special padding word used to make all the inputs the same length when batching together sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c40e9-2e05-47ee-8d0d-b6db707de6b4",
   "metadata": {},
   "source": [
    "### Architectures vs. checkpoints\n",
    "\n",
    "- **Architecture**: This is the skeleton of the model — the definition of each layer and each operation that happens within the model.\n",
    "- **Checkpoints**: These are the weights that will be loaded in a given architecture.\n",
    "- **Model**: This is an umbrella term that isn’t as precise as “architecture” or “checkpoint”: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity.\n",
    "\n",
    "For example, BERT is an architecture while bert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say “the BERT model” and “the bert-base-cased model.”\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ccd464-49da-4442-a68e-902f5496d9fc",
   "metadata": {},
   "source": [
    "While transformer models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b566f2-180e-44f9-9ed4-28f5753bf9c2",
   "metadata": {},
   "source": [
    "# **Using Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86636b8a-7d91-4f94-9e3a-e01ad91da432",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The library’s main features are:\n",
    "\n",
    "- Ease of use: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.\n",
    "- Flexibility: At their core, all models are simple PyTorch nn.Module or TensorFlow tf.keras.Model classes and can be handled like any other models in their respective machine learning (ML) frameworks.\n",
    "- Simplicity: Hardly any abstractions are made across the library. The “All in one file” is a core concept: a model’s forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f8df2-a8d5-40c0-9e89-45bfa8ab03d1",
   "metadata": {},
   "source": [
    "## Behind the pipeline\n",
    "\n",
    "The pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:\n",
    "\n",
    "![Pipeline](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eee44b-d2af-4d4f-87a8-5b323dc8db29",
   "metadata": {},
   "source": [
    "### Preprocessing with a tokenizer\n",
    "\n",
    "First step of our pipeline\n",
    "- convert the text inputs into numbers that the model can make sense of.\n",
    "- use a tokenizer, which will be responsible for:\n",
    "  - Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "  - Mapping each token to an integer\n",
    "  - Adding additional inputs that may be useful to the model\n",
    "\n",
    "\n",
    "- All this preprocessing needs to be done in exactly the same way as when the model was pretrained.\n",
    "- We use the AutoTokenizer class and its `from_pretrained()` method.\n",
    "- Using the checkpoint name of our model, it will automatically fetch the data associated with the model’s tokenizer and cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb472060-e4f2-4a1d-b5e9-0857a0b6b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ad50af7-90cb-47af-84af-d893a88555fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a710f0-e722-4869-91e5-0c3bef12ee3a",
   "metadata": {},
   "source": [
    "### Going through the model\n",
    "Transformers provides an `AutoModel` class which also has a `from_pretrained()` method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0eba29b3-ab1a-4080-8e8e-e0ff627c7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49643b5-31b0-497e-bef3-0efb89f33734",
   "metadata": {},
   "source": [
    "In this code snippet, we have downloaded the same checkpoint we used in our pipeline before (it should actually have been cached already) and instantiated a model with it.\n",
    "\n",
    "given some inputs, it outputs what we’ll call hidden states, also known as features.\n",
    "\n",
    "For each model input, we’ll retrieve a high-dimensional vector representing the **contextual understanding of that input by the Transformer model**.\n",
    "\n",
    "The hidden states are usually inputs to another part of the model, known as the head.\n",
    "\n",
    "- the different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc26e894-2bd4-4aa4-aafd-def1bf76c0a1",
   "metadata": {},
   "source": [
    "given some inputs, it outputs what we’ll call hidden states, also known as features.\n",
    "\n",
    "For each model input, we’ll retrieve a high-dimensional vector representing the **contextual understanding of that input by the Transformer model**.\n",
    "\n",
    "The hidden states are usually inputs to another part of the model, known as the head.\n",
    "\n",
    "- the different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db96343-68bb-48d6-8f09-ec9d434188f2",
   "metadata": {},
   "source": [
    "#### A high-dimensional vector?\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "- Batch size: The number of sequences processed at a time (2 in our example).\n",
    "- Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "- Hidden size: The vector dimension of each model input.\n",
    "\n",
    "The hidden size can be very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f48c4ff2-2f19-47a1-94b1-41572f3ca4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fddddf-7ea2-4a1e-b96a-0e019b3d6fdd",
   "metadata": {},
   "source": [
    "#### Model Heads\n",
    "- Model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension.\n",
    "- The output of the Transformer model is sent directly to the model head to be processed.\n",
    "\n",
    "There are many different architectures available in Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:\n",
    "\n",
    "- *Model (retrieve the hidden states)\n",
    "- *ForCausalLM\n",
    "- *ForMaskedLM\n",
    "- *ForMultipleChoice\n",
    "- *ForQuestionAnswering\n",
    "- *ForSequenceClassification\n",
    "- *ForTokenClassification\n",
    "- and others \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85d614d7-f0d1-4984-a064-f0c32de3b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28aec8-fd4d-44c7-8045-53ca8b0428b5",
   "metadata": {},
   "source": [
    "- if we look at the shape of our outputs, the dimensionality will be much lower:\n",
    "- the model head takes as input the high-dimensional vectors and outputs vectors containing two values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e5dcc43-b57e-4099-898f-2571af082279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb38720d-c5e8-4438-ad58-b927efaa6b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcf49c29-ce27-419a-bf12-24ca17b32c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2c4fd-8444-4820-ace8-81da483b6d87",
   "metadata": {},
   "source": [
    "## Models\n",
    "### Creating a Transformer\n",
    "\n",
    "to initialize a BERT model\n",
    "- load a configuration object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6823ceb-854c-48c0-bf78-98902b594c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8635d3c6-5d3d-493b-9a2c-38f397c23ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b1927-ca79-49fb-930b-f8f6d44b2bc7",
   "metadata": {},
   "source": [
    "#### Different loading methods\n",
    "\n",
    "Creating a model from the default configuration initializes it with random values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42b4d871-7517-4a3a-8fdf-34ff6ca36a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daa9d228-9d43-4e8f-8be6-2965734768b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|████████████████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 46.5kB/s]\n",
      "C:\\Users\\Aavash\\anaconda3\\envs\\research\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aavash\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 436M/436M [01:45<00:00, 4.12MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "193110b6-c1ac-49ed-9907-b994edd5621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/content/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beafdcf4-8da5-46e7-a6f6-76b7ae14d2a6",
   "metadata": {},
   "source": [
    "### Using a Transformer model for inference\n",
    "Before we discuss tokenizers, let’s explore what inputs the model accepts.\n",
    "Let’s say we have a couple of sequences:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1793bd8e-2890-4aa1-90b2-f0eb3a8a44c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83563438-9391-450d-9cfe-62314b2e3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a330bc-29f4-434b-a37e-f58f463c389f",
   "metadata": {},
   "source": [
    "This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71898820-dee9-4638-80ae-63e1975b7966",
   "metadata": {},
   "source": [
    "#### Using the tensors as inputs to the model\n",
    "\n",
    "we just call the model with the inputs:\r\n",
    "\r\n",
    "`output = model(model_inputs)\n",
    "\n",
    "While the model accepts a lot of different arguments, only the input IDs are necessary.`\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a608d0-88fe-44b6-b2af-d7d9b5abd09e",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "- Translate text into data that can be processed by the model.\n",
    "- Models can only process numbers, so tokenizers need to convert our text inputs to numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc730b-4597-4c72-9b87-d853cb5b5712",
   "metadata": {},
   "source": [
    "### Word-based\n",
    "in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\n",
    "\n",
    "![Word-based](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad1959-d512-462f-ac31-10ac81a16777",
   "metadata": {},
   "source": [
    "There are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python’s split() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93c642a5-49a5-4491-a60c-251bdfa994b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52192c-1192-4d95-bf37-cd7e941f7cbb",
   "metadata": {},
   "source": [
    "- There are also variations of word tokenizers that have extra rules for punctuation.\n",
    "- Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.\n",
    "\n",
    "- If we want to completely cover a language with a word-based tokenizer, we’ll need to have an identifier for each word in the language, which will generate a huge amount of tokens.\n",
    "\n",
    "- Furthermore, words like “dog” are represented differently from words like “dogs”, and the model will initially have no way of knowing that “dog” and “dogs” are similar:\n",
    "\n",
    "- Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the “unknown” token, often represented as ”[UNK]” or ””.  Generally a bad sign when a lot of these tokens are produced as the tokenizers wasn’t able to retrieve a sensible representation of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74012e75-0999-4753-a80d-0af8e882af84",
   "metadata": {},
   "source": [
    "One way to reduce the amount of unknown tokens is to go one level deeper, using a character-based tokenizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405920b-5efe-4978-8874-b4907999cdb9",
   "metadata": {},
   "source": [
    "### Character-Based\n",
    "\n",
    "Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n",
    "\n",
    "- The vocabulary is much smaller.\n",
    "- There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\n",
    "\n",
    "![Character-based](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5b4bc-ce4f-4baa-bc3e-8fadaa9f40ea",
   "metadata": {},
   "source": [
    "- Since the representation is now based on characters rather than words, one could argue that, intuitively, it’s less meaningful.\n",
    "- each character doesn’t mean a lot on its own.\n",
    "- However, this again differs according to the language;\n",
    "  - in Chinese, for example, each character carries more information than a character in a Latin language.\n",
    "\n",
    "Another thing to consider is that we’ll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters.\n",
    "\n",
    "To get the best of both worlds, we can use a third technique that combines the two approaches: subword tokenization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e51958-3375-4f5e-afea-ed6a14466d9b",
   "metadata": {},
   "source": [
    "### Subword Tokenization\n",
    "Frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n",
    "\n",
    "-  “annoyingly” might be considered a rare word and could be decomposed into “annoying” and “ly”.\n",
    "- These are both likely to appear more frequently as standalone subwords.\n",
    "- while at the same time the meaning of “annoyingly” is kept by the composite meaning of “annoying” and “ly”.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a39c6-91e2-4afa-9b40-54e12b2b3345",
   "metadata": {},
   "source": [
    "example showing how a subword tokenization algorithm would tokenize the sequence “Let’s do tokenization!“:\n",
    "\n",
    "This approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\r\n",
    "\r\n",
    "\r\n",
    "![Subword](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d68ecc-628b-4145-afd3-d31d3861fb7a",
   "metadata": {},
   "source": [
    "### Loading and saving\n",
    "\n",
    "Loading and saving tokenizers is as simple as it is with models.\n",
    "- it’s based on the same two methods: `from_pretrained()` and `save_pretrained()`.\n",
    "\n",
    "These methods will load or save the algorithm used by the tokenizer as well as its vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d26ef3a6-ab76-457c-abc9-c4bc5be63d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 29.0/29.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|█████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 924kB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████| 436k/436k [00:00<00:00, 2.25MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f9431f6-d1bb-49c9-a2d4-3e3b5d96a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6916505a-f7e7-4677-ac93-e70c906af1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "607af9ff-3158-4368-8a75-218ca37fa66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer2\\\\tokenizer_config.json',\n",
       " 'tokenizer2\\\\special_tokens_map.json',\n",
       " 'tokenizer2\\\\vocab.txt',\n",
       " 'tokenizer2\\\\added_tokens.json',\n",
       " 'tokenizer2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb66307-c2bb-4318-85e9-0358a057785f",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "- Translating text to numbers is known as encoding.\n",
    "- Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.\n",
    "- the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called tokens.\n",
    "- The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model.\n",
    "- To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "852ccfe5-ff40-4043-8c9f-28d5bf5858a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "689cfb1d-87fd-4c22-b779-4502d9f547e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca25ce96-f848-4180-b6de-abc71070fb55",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "\n",
    "Decoding is going the other way around:\r\n",
    "- from vocabulary indices, we want to get a string.\r\n",
    "- can be done with the `decode()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eac148e7-15e6-43c0-8a47-b9da3b95d37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95944a76-994b-4e52-92a9-850ae992020e",
   "metadata": {},
   "source": [
    "Decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence.\n",
    "## Handling multiple sequences\n",
    "- How do we handle multiple sequences?\n",
    "- How do we handle multiple sequences of different lengths?\n",
    "- Are vocabulary indices the only inputs that allow a model to work well?\n",
    "- Is there such a thing as too long a sequence?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba731e61-cebf-414b-a961-6d269d932ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8be1d982-573c-4dc1-887c-a5519851c7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e683d2-0063-4464-bd66-0c95864bb8c3",
   "metadata": {},
   "source": [
    "*Batching* is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e19fbdb9-af88-406e-90e7-f61c3f6760d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [ids, ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e557138c-6091-43e3-acb8-26948d01cb87",
   "metadata": {},
   "source": [
    "This is a batch of two identical sequences!\n",
    "\n",
    "- Batching allows the model to work when you feed it multiple sentences.\n",
    "- Using multiple sequences is just as simple as building a batch with a single sequence.\n",
    "\n",
    "When you’re trying to batch together two (or more) sentences, they might be of different lengths. To work around this problem, we usually pad the inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f613d-9903-4d18-94f6-96baa22a43c6",
   "metadata": {},
   "source": [
    "### Padding the inputs\n",
    "The following list of lists cannot be converted to a tensor:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36f9ef58-4295-4c3d-969d-33ecab061ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c68450-3f43-48d1-afcb-2ffa4b5a94ed",
   "metadata": {},
   "source": [
    "We’ll use *padding* to make our tensors have a rectangular shape.\n",
    "\n",
    "Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values.\n",
    "For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words. In our example, the resulting tensor looks like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "347cd759-226e-4306-b825-5fb30a8d64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1436fd0-27e0-4072-ba0f-c4b9ee2811d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a767f2b-f5d9-4a19-b6e6-465fa82f9f79",
   "metadata": {},
   "source": [
    "### Attention Masks\n",
    "*Attention Masks* aretensors with the exact same shape as the input IDs tensor, filled with 0s and 1s:\n",
    "\n",
    "1s indicate the corresponding tokes should be attended to, and 0s indicate the corresponding tokens should not be attended to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0151b20b-1581-4b6c-837f-61926b137fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e455248-f0fd-41f0-b377-f38002121238",
   "metadata": {},
   "source": [
    "### Longer sequences\n",
    "\n",
    "With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n",
    "\n",
    "- Use a model with a longer supported sequence length.\n",
    "- Truncate your sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255255d-bdf7-4513-abe3-4b47321d64b8",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "The 🤗 Transformers API can handle  tokenization, conversion to input IDs, padding, truncation, and attention masks for us with a high-level function that we’ll dive into here.\n",
    "\n",
    "When you call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model:\n",
    "\n",
    "When you call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf001e4d-defb-4ceb-844a-e5adffa7e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2dc36f02-d9b6-4638-8ec9-d926ad0e015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0216db3b-5a9b-4697-9cb1-fe4d9b29bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "257733d1-f986-4524-95df-0290c9b1abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "34f220e3-9760-42ef-b65c-a14ef4aac6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)\n",
    "\n",
    "model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f9d6ccf-fbd0-42ff-a0ae-e5c4b1980086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4ac1ed4-614a-45c3-868c-5685bd46e18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03a4c35f-6bea-46f3-b14b-74f3e538ee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
       "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c146ea-48f9-4844-a864-f13976da872e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
